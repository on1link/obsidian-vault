  * Attention is motivated by how give importance to different regions of a image or correlated word in a phrase. It can be interpreted as a vector of weights of importance : To predict or infer an element, this is estimated using a attention vector which tell us how strong are correlated the element with others elements and take the sum of these weighed values by the vector as an approximation of the label. Also can be used as a tool to understand the behavior of neural networks.
  * The general definition [[Bahdanau, D.2015 Neural machine translation by jointly learning to align and translate]] is as follows
  * Is the mapping of a sequence $K$ to a distribution $a$ 
	  * Where $K$ are the "keys" that can be a word or an embedding of characters of a document, or the internal state of a [[Machine Learning/Deep Learrning/RNN1|RNN1]]. 
	  * $q$ the "query" used as a reference to get the attention score, can be a hidden state or a range of text embedding. 
	  * Finally $V$ the "values", is where the attention is applied, with $K$ each element can offer two, possible different, interpretations of the same entity.