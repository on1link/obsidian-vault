  * ## First view
    * 1. Categoría: ¿Qué tipo de articulo es éste? ¿Un articulo de medición? ¿Un análisis de un sistema existente? ¿Una descripción de un prototipo de investigación?
      * Is about a survey about that try to summarized all the existing efforts to improve the efficiency of Vanilla Transformer based on the three main issues on the self-attention module
        * Computation complexity
        * Memory Complexity
        * Inference Problems
    * 2. Contexto: ¿Con qué otros artículos está relacionado? ¿Qué bases teóricas se usaron para analizar el problema?
      * It's related with all the Efficient Transformers models of the vast quantity of models that currently exists in the state-of-the-art.
      * The theorical background is based on complexity and memory use of the Transformer and its variation models
    * 3. Exactitud (*correctness*): ¿Los supuestos parecen ser validos?
      * Yes, because it's a problem reported in several papers about the problems of the self-attention module
    * 4. Contribuciones: ¿Cuáles son las principales contribuciones del artículo?
      * The main contribution is to gain sight with a high level description of the available models and a taxonomy to see their similarities and differences. Showing the current lines of research in parallel and orthogonal improvements to the Transformer models.
      * Summarize the problem with the Vanilla Transformer
      * Issues the problems with replicating the models, from using diferent benchmarks to the difficult to compare models because they are for different tasks.
    * 5. Claridad: ¿Está bien escrito el documento?
      * Yes, it's define the core issue, the taxonomy, and a brief review of the existing models to date.
  * ## Second View

  * ### References
    * [[Vaswani, A.2017 Attention is all you need]]
    * [[Zhou, H.2021 Informer Beyond Efficient Transformer for Long Sequence Time-Series Forecasting]]
    * + [[Dai, Z.2019 Transformer-XL Attentive language models beyond a fixed-lenght context]]
    * [[Bahdanau, D.2014 Neural machine translation by jointly learning to align and translate]]
    * + [[Child, R.2019 Generating long sequences with sparse transformers]]
    * + [[Guo, Y.2019b Nat Neural architecture transformer for accurate and compact architectures]]
    * + [[Kitaev, N.2020 Reformer The efficient transformer]]
    * + [[Zaheer, M2020 Big bird Transformers for longer sequences]]
    * + [[Devlin, J.2018 Bert Pre-training of deep bidirectional transformers for language understanding]]
    * + [[Lei, J.2016 Layer normalization]]
