  * Element presentation Walmart Machine Learning Platform (wmlink/element)
    * #mlplatform on slack
    * element-solution@email.wal-mart.com
    * Moving code to spark on elment
      * Jupyter note and connectivty to Hadoop cluster
      * Partition data and add loigc for forecasting per parittion
      * Make the preferred libraries available on al lpsark worker node with all its dependeci
    * How create a spark notebook in element
      * Notebook tabs
      * Create Notebook
      * Multiple runtime (python, r spark/hive/ gpu)
      * Select spark
      * Resource Allotment (MOST IMPORTANT PART)
      * Driver -> number of cores and memory
      * Executor -> number of core -> memory -> number of executor (e.g. 2 cores per executor )
      * Additional Spark Properties
      * EXecutors and cores
        * Number of partitions to match executors * cores (20p = 4 execturo * 5 cores)
        * Start with 5 cores per exectuor
        * Can 5 tasks fit in one executor's memory. Change cores/executor accordingly
      * Memory
        * Number of records to be hanlded in single partition
        * Ballparl figure = 1 record size * avg number of records per unique combination
    * Use case improvements
      * International Tech
        * XGBoost was earlir running on a python kernel/notebook with 75 GB RAM, 11 cores and 1 GPU on Element. Earlier runtime varied from 5-8 hrs depending on the size of deparment. After porting to spark with UDF it takes 2.5-3.5 hrs. Prophet model has also shown improvements in the runtime.
        * Supply Chain - Forecasting for 91 days for 90k combinations, data run time reduced from 5 hours to 12 mins using prophet and PySpark