

  * ## References
    * [[Dai, Z.2019 Transformer-XL Attentive language models beyond a fixed-lenght context]]
    * [[Vaswani, A.2017 Attention is all you need]]
    * [[Koutnik, J.2014 A clockwork rnn]]